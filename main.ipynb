{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1I_LSvPxAZhPWjPwRk1Hpnsgoz-mk1p6K","authorship_tag":"ABX9TyNQ6opa0ls6io25WKZGjQ6q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2Ibq3hAhuhg3"},"source":["!pip install kaggle --upgrade\n","!mkdir /root/.kaggle/\n","!cp '/content/drive/My Drive/kaggle.json' /root/.kaggle/   # kaggl.json위치 지정\n","!chmod 600 /root/.kaggle/kaggle.json\n","!kaggle datasets download -d dhuh137/hmdb51\n","!unzip hmdb51.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCQF7f1vuwrs"},"source":["import os\n","import numpy as np\n","\n","for foldername in os.listdir('/content/HMDB51_NUMPY/fold_1/training/0'):\n","  for filename in os.listdir('/content/HMDB51_NUMPY/fold_1/training/0' + '/'  + foldername):\n","    temp = np.load('/content/HMDB51_NUMPY/fold_1/training/0' + '/'  + foldername + '/' + filename)\n","    if temp.shape != (11,64,64,3):\n","      print(filename)\n","      os.remove('/content/HMDB51_NUMPY/fold_1/training/0' + '/'  + foldername + '/' + filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nG_Marw8uwt8"},"source":["import numpy as np\n","import keras\n","\n","class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n","                 n_classes=10, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.labels = labels\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n","        y = np.empty((self.batch_size), dtype=int)\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            # Store sample\n","              \n","              X[i,] = np.load(ID)\n","\n","              # Store class\n","              y[i] = self.labels[ID]\n","\n","        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PH0rK9YJuwwG"},"source":["import numpy as np\n","import os\n","from keras.models import Sequential\n","\n","\n","# Parameters\n","params = {'dim': (11,64,64),\n","          'batch_size': 2,\n","          'n_classes': 51,\n","          'n_channels': 3,\n","          'shuffle': True}\n","\n","# Datasets\n","#partition = {'train': ['id-1', 'id-2', 'id-3'], 'validation': ['id-4']}\n","\n","DATA_PATH = '/content/HMDB51_NUMPY/fold_1/training/0'\n","labels = {}\n","partitions = {'train':[]}\n","for classname in os.listdir(DATA_PATH):\n","  for video in os.listdir(DATA_PATH + '/' + classname):\n","    id = DATA_PATH + '/' + classname + '/' + video\n","    partitions['train'].append(id)\n","    labels[id] = classname\n","\n","\n","\n","# Generators\n","training_generator = DataGenerator(partitions['train'], labels, **params)\n","#validation_generator = DataGenerator(partition['validation'], labels, **params)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfXZDL1WuwyU"},"source":["import os\n","os.chdir('/content/drive/My Drive/종합설계/myslowfast')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g5Ky35zuw0N"},"source":["from util.utils import get_optimizer\n","from util import opts\n","from model.model import SlowFast_body\n","from util.Callback import create_callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnduf4JxDbPd"},"source":["opt = opts.opts()\n","print(opt.root_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_IeRpwHU8Ubv"},"source":["opt = opts.opts()\n","model = SlowFast_body(input_shape = (11,64,64,3),num_classes=51)\n","callback = create_callbacks(opt,len(training_generator),model)\n","optimizer = get_optimizer(opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_8KXq4wAhK6"},"source":["import tensorflow as tf\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","# Train model on dataset\n","with tf.device('/device:GPU:0'):\n","  hist = model.fit_generator(generator=training_generator,steps_per_epoch=len(training_generator),epochs=50,callbacks=callback)\n","\n","model.save_weights(os.path.join(os.path.join(opt.root_path, opt.result_path), 'trained_weights_final.h5'))\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"msZTyiNIAiL6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESCyDTBbuw2P"},"source":["import os\n","import math\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.utils import multi_gpu_model\n","from model import nets\n","from opts import parse_opts\n","from utils import get_optimizer, SGDRScheduler_with_WarmUp, TrainPrint, PrintLearningRate, ParallelModelCheckpoint\n","from dataset.dataset import DataGenerator\n","\n","\n","def create_callbacks(opt, steps_per_epoch, model=None):\n","    log_dir = os.path.join(opt.root_path, opt.log_dir)\n","    if not os.path.exists(log_dir):\n","        os.mkdir(log_dir)\n","    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True)\n","\n","    result_path = os.path.join(opt.root_path, opt.result_path)\n","    if not os.path.exists(result_path):\n","        os.mkdir(result_path)\n","\n","    if model is not None:\n","        checkpoint = ParallelModelCheckpoint(model, os.path.join(result_path, 'ep{epoch:03d}-val_acc{val_acc:.2f}.h5'),\n","                                    monitor='val_acc', save_weights_only=True, save_best_only=True, period=1)\n","    else:\n","        checkpoint = ModelCheckpoint(os.path.join(result_path, 'ep{epoch:03d}-val_acc{val_acc:.2f}.h5'),\n","                                    monitor='val_acc', save_weights_only=True, save_best_only=True, period=1)\n","    early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10)\n","    learning_rate_scheduler = SGDRScheduler_with_WarmUp(0, opt.lr, steps_per_epoch, lr_decay=opt.lr_decay, \n","                                                        cycle_length=opt.cycle_length, multi_factor=opt.multi_factor,\n","                                                        warm_up_epoch=opt.warm_up_epoch)\n","    #training_print = TrainPrint(steps_per_epoch, opt.epochs)\n","    print_lr = PrintLearningRate()\n","\n","    return [tensorboard, learning_rate_scheduler, print_lr, checkpoint, early_stopping]\n","    \n","\n","\n","def train(opt):\n","    K.clear_session()\n","    video_input = Input(shape=(None, None, None, 3))\n","    model = nets.network[opt.network](video_input, num_classes=opt.num_classes)\n","    print(\"Create {} model with {} classes\".format(opt.network, opt.num_classes))\n","\n","    if opt.pretrained_weights is not None:\n","        model.load_weights(opt.pretrained_weights)\n","        print(\"Loading weights from {}\".format(opt.pretrained_weights))\n","\n","    optimizer = get_optimizer(opt)\n","\n","    train_data_generator = DataGenerator(opt.data_name, opt.video_path, opt.train_list, opt.name_path, \n","                                        'train', opt.batch_size, opt.num_classes, True, opt.short_side, \n","                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video)\n","    val_data_generator = DataGenerator(opt.data_name, opt.video_path, opt.val_list, opt.name_path, 'val', \n","                                        opt.batch_size, opt.num_classes, False, opt.short_side, \n","                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video)\n","    \n","    \n","    callbacks = create_callbacks(opt, max(1, train_data_generator.__len__()), model)\n","\n","    if len(opt.gpus) > 1:\n","        print('Using multi gpus')\n","        parallel_model = multi_gpu_model(model, gpus=len(opt.gpus))\n","        parallel_model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n","        parallel_model.fit_generator(train_data_generator, steps_per_epoch=max(1, train_data_generator.__len__()),\n","                            epochs=opt.epochs, validation_data=val_data_generator, validation_steps=max(1, val_data_generator.__len__()),\n","                            workers=opt.workers, callbacks=callbacks)\n","    else:\n","        model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n","        model.fit_generator(train_data_generator, steps_per_epoch=max(1, train_data_generator.__len__()),\n","                            epochs=opt.epochs, validation_data=val_data_generator, validation_steps=max(1, val_data_generator.__len__()),\n","                            workers=opt.workers, callbacks=callbacks)\n","    model.save_weights(os.path.join(os.path.join(opt.root_path, opt.result_path), 'trained_weights_final.h5'))\n","\n","    \n","if __name__==\"__main__\":\n","    opt = parse_opts()\n","    print(opt)\n","    if len(opt.gpus) > 1:\n","        os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(map(str, opt.gpus))\n","    train(opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfi_YP5D3TVI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmPKXiHPuw4s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8THYzXQ_uw6h"},"source":[""],"execution_count":null,"outputs":[]}]}