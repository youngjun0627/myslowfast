# -*- coding: utf-8 -*-
"""callback.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VKaMZtNGRC-ho2dz_AJFIoXnZ_p7w2PY
"""

from util.utils import get_optimizer, SGDRScheduler_with_WarmUp, TrainPrint, PrintLearningRate, ParallelModelCheckpoint
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
import os

def create_callbacks(opt, steps_per_epoch, model=None):
    print(opt.root_path)
    log_dir = os.path.join(opt.root_path, opt.log_dir)
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)
    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True)
    

    result_path = os.path.join(opt.root_path, opt.result_path)
    if not os.path.exists(result_path):
        os.mkdir(result_path)

    if model is not None:
      
        checkpoint = ParallelModelCheckpoint(model, os.path.join(result_path, 'ep{epoch:03d}-val_accuracy{val_accuracy:.2f}.h5'),
                                    monitor='val_accuracy', save_weights_only=True, save_best_only=True, period=1)
    else:
        checkpoint = ModelCheckpoint(os.path.join(result_path, 'ep{epoch:03d}-val_accuracy{val_accuracy:.2f}.h5'),
                                    monitor='val_accuracy', save_weights_only=True, save_best_only=True, period=1)
    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20)
    learning_rate_scheduler = SGDRScheduler_with_WarmUp(0, opt.lr, steps_per_epoch,  lr_decay=opt.lr_decay,
                                                        cycle_length=opt.cycle_length, multi_factor=opt.multi_factor,
                                                        warm_up_epoch=opt.warm_up_epoch)
    #training_print = TrainPrint(steps_per_epoch, opt.epochs)
    print_lr = PrintLearningRate()

    return [tensorboard, learning_rate_scheduler, print_lr, checkpoint, early_stopping]