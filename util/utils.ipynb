{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils.ipynb","provenance":[],"private_outputs":true,"mount_file_id":"1eE3PlLm0j0xco87fKI6zGjJcrTG53w1C","authorship_tag":"ABX9TyO/aOiInhorKj8oTv88V5yd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XGUw7UY1tOyX"},"source":["\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n","from tensorflow.keras import backend as K\n","\n","\n","\n","def get_optimizer(opt):\n","    if opt.optimizer == 'SGD':\n","        optimizer = SGD(lr=opt.lr, momentum=opt.momentum, decay=opt.weight_decay)\n","    elif opt.optimizer == 'Adam':\n","        optimizer = Adam(lr=opt.lr, decay=opt.weight_decay)\n","\n","    return optimizer\n","\n","class ParallelModelCheckpoint(ModelCheckpoint):\n","    def __init__(self, model, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1):\n","        self.single_model = model\n","        super(ParallelModelCheckpoint, self).__init__(filepath, monitor, verbose, save_best_only, save_weights_only, mode, period)\n","\n","    def set_model(self, model):\n","        super(ParallelModelCheckpoint, self).set_model(self.single_model)\n","\n","\n","class SGDRScheduler_with_WarmUp(Callback):\n","\n","    def __init__(self, min_lr, max_lr, steps_per_epoch, lr_decay=1, cycle_length=10, multi_factor=2, warm_up_epoch=5):\n","        self.min_lr = min_lr\n","        self.max_lr = max_lr\n","        self.steps_per_epoch = steps_per_epoch\n","        self.lr_decay = lr_decay\n","        self.cycle_length = cycle_length\n","        self.multi_factor = multi_factor\n","        self.warm_up_epoch = warm_up_epoch\n","\n","        self.is_warming = True\n","\n","        self.history = {}\n","\n","    def sgdr_lr(self):\n","        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n","        print('', fraction_to_restart)\n","        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n","        return lr\n","\n","    def warm_lr(self):\n","        lr = self.max_lr * (self.warm_up_batch / (self.steps_per_epoch * self.warm_up_epoch)) * (self.warm_up_batch / (self.steps_per_epoch * self.warm_up_epoch))\n","        return lr\n","\n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","        self.warm_up_batch = 1\n","        K.set_value(self.model.optimizer.lr, self.warm_lr())\n","\n","    def on_batch_end(self, batch, logs={}):\n","        logs = logs or {}\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        for k, v in logs.items():\n","            self.history.setdefault(k ,[]).append(v)\n","        \n","        if self.is_warming:\n","            self.warm_up_batch += 1\n","            K.set_value(self.model.optimizer.lr, self.warm_lr())\n","        else:\n","            self.batch_since_restart += 1\n","            K.set_value(self.model.optimizer.lr, self.sgdr_lr())\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        if epoch == self.warm_up_epoch:\n","            self.is_warming = False\n","            self.batch_since_restart = 0\n","            self.next_restart = self.cycle_length + epoch\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch >= self.warm_up_epoch:\n","            if epoch + 1 == self.next_restart:\n","                self.batch_since_restart = 0\n","                self.cycle_length = np.ceil(self.cycle_length * self.multi_factor)\n","                self.next_restart += self.cycle_length\n","                self.max_lr *= self.lr_decay\n","                self.best_weights = self.model.get_weights()\n","\n","    def on_train_end(self, logs={}):\n","        self.model.set_weights(self.best_weights)\n","\n","class PrintLearningRate(Callback):\n","    def on_batch_end(self, batch, logs={}):\n","        logs = logs or {}\n","        if batch > 0:\n","            print(' - lr: %.6f'%K.get_value(self.model.optimizer.lr))\n","\n","class TrainPrint(Callback):\n","    def __init__(self, steps_per_epoch, max_epoch):\n","        self.steps_per_epoch = steps_per_epoch\n","        self.max_epoch = max_epoch\n","        self.log = 'epoch [%.3d]/[%.3d] batch [%d/%d] loss %.4f lr %.6f acc %.2f'\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        self.epoch = int(epoch)\n","\n","    def on_batch_end(self, batch, logs={}):\n","        logs = logs or {}\n","        # loss = float(logs['loss'])\n","        # lr = float(K.get_value(se))\n","        print(self.log%(self.epoch, self.max_epoch, batch, self.steps_per_epoch, logs['loss'], K.get_value(self.model.optimizer.lr), logs['acc']))\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        logs = logs or {}\n","        if 'val_loss' and 'val_acc' in logs.keys():\n","            print('Validate on epoch {} : loss {} acc {}'.format(epoch, logs['val_loss'], logs['val_acc']))\n","\n"],"execution_count":null,"outputs":[]}]}